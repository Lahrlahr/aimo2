{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T01:44:34.216475Z",
     "iopub.status.busy": "2025-04-09T01:44:34.216127Z",
     "iopub.status.idle": "2025-04-09T01:45:25.498422Z",
     "shell.execute_reply": "2025-04-09T01:45:25.497736Z",
     "shell.execute_reply.started": "2025-04-09T01:44:34.216433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 01:45:20,863\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/lmdeploy-package')\n",
    "from lmdeploy import pipeline, TurbomindEngineConfig, GenerationConfig\n",
    "from transformers import set_seed\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "from vllm import LLM, SamplingParams\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict, Any\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "random_seed = int(time.time()) % 10000\n",
    "set_seed(random_seed)\n",
    "print(random_seed)\n",
    "cutoff_time = time.time() + (4 * 60 + 57) * 60\n",
    "global_start_time = time.time()\n",
    "\n",
    "# Initialize with default values - will be adjusted dynamically based on progress\n",
    "speed = 3\n",
    "num_samples = 15\n",
    "max_batch_size = 20\n",
    "if_only_cot = False\n",
    "\n",
    "# Define sample size mapping based on speed\n",
    "SPEED_TO_SAMPLES = {\n",
    "    1: 10,  # Fastest: fewer samples\n",
    "    2: 12,\n",
    "    3: 15,  # Default\n",
    "    4: 16,\n",
    "    5: 17   # Slowest: more samples\n",
    "}\n",
    "\n",
    "llm_model_pth_14 = \"/kaggle/input/deepseek-r1-14b-awq-turobmind/other/default/1/deepseek-14b\"\n",
    "llm_model_pth_14_3_13 = \"/kaggle/input/dpsk-14b-sft-dpo-3-13-awq-tb/keras/default/1/dpsk-14b-sft-dpo4-3-13-awq-tb\"\n",
    "llm_model_pth_14_3_16 = \"/kaggle/input/dpsk-14b-sft-dpo-3-16-awq-tb/keras/default/1/dpsk-14b-sft-dpo2-3-16-awq-tb\"\n",
    "llm_model_pth_14_3_30 = \"/kaggle/input/dpsk-14b-epoch2-repkv-3-30-tb/keras/default/1/dpsk-14b-epoch2-repkv-3-30-tb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T01:45:25.500751Z",
     "iopub.status.busy": "2025-04-09T01:45:25.500099Z",
     "iopub.status.idle": "2025-04-09T01:45:25.506338Z",
     "shell.execute_reply": "2025-04-09T01:45:25.505637Z",
     "shell.execute_reply.started": "2025-04-09T01:45:25.500718Z"
    },
    "papermill": {
     "duration": 15.076705,
     "end_time": "2024-10-27T05:59:33.712437",
     "exception": false,
     "start_time": "2024-10-27T05:59:18.635732",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/lmdeploy-package')\n",
    "# for only cot\n",
    "thoughts = [\n",
    "    'Please use chained reasoning to put the answer in \\\\boxed{}.',\n",
    "    'Please reflect and verify while reasoning and put the answer in \\\\boxed{}.',\n",
    "    'Solve the following problem using concise and clear reasoning by placing the answer in \\\\boxed{}.',\n",
    "    'You are a helpful and reflective maths assistant, you reflect on each step of your reasoning.Please reason step by step to put the answer in \\\\boxed{}.',\n",
    "    'You are the smartest maths expert in the world, please spike this question and put the answer in \\\\boxed{}.',\n",
    "    # 'You are the cleverest maths expert in the world, please work out this question and put the answer in \\\\boxed{}.'\n",
    "]\n",
    "new_thoughts = [\n",
    "    'Please use chained reasoning to put the answer in \\\\boxed{}.',\n",
    "    'Please reflect and verify while reasoning and put the answer in \\\\boxed{}.',\n",
    "    'Solve the following problem using concise and clear reasoning by placing the answer in \\\\boxed{}.',\n",
    "    'You are a helpful and reflective maths assistant, you reflect on each step of your reasoning.Please reason step by step to put the answer in \\\\boxed{}.',\n",
    "    'You are the smartest maths expert in the world, please spike this question and put the answer in \\\\boxed{}.',\n",
    "    \"Scrutinize every logical connection, validate intermediate steps, and conclude with a boxed solution: \\boxed{}.\", \n",
    "    \"Adopt the mindset of a world-class problem-solver: methodically dissect the problem and present the answer in \\boxed{}.\",\n",
    "    \"Deconstruct the challenge systematically, ensuring coherence at each phase, and finalize with \\boxed{}.\",\n",
    "    'You are a helpful maths assistant. Please reason step by step to put the answer in \\\\boxed{}.',\n",
    "    'Please use chained reasoning to put the answer in \\\\boxed{}.',\n",
    "    'Solve the following problem using concise and clear reasoning by placing the answer in \\\\boxed{}.',\n",
    "    'You are a helpful maths assistant. Please reason step by step to put the answer in \\\\boxed{}.',\n",
    "    'Drive your mathematical analysis forward with purpose and certainty, concluding in \\boxed{}.',\n",
    "    'Develop your solution through focused mathematical steps and place your answer in \\boxed{}.',\n",
    "    'Execute your problem-solving strategy with clarity and conviction, presenting the result in \\boxed{}.',\n",
    "   ]\n",
    "thoughts_cot = (\n",
    "    # '\\nDo not need to verify the answer, save time.'\n",
    "    \"\\n You excel at reasoning.\"\n",
    "    '\\n You must put the final answer in \\\\boxed{} before </think>.'\n",
    "    '\\n The final answer should modulo 1000.'\n",
    "    \"\\n Avoid duplication and improve efficiency.\"\n",
    ")\n",
    "# for cot + coder\n",
    "thoughts_code = (\n",
    "    # '\\n Do not need to verify the answer, save time.'\n",
    "    '\\n You excel at coding.'\n",
    "    '\\n Provide the python code, avoid redundant analysis'\n",
    "    '\\n For difficult problems, don’t think too long, just give the code as soon as possible.'\n",
    "    '\\n The final answer should modulo 1000 and must be an integer.'\n",
    "    '\\n There is only one answer for each question.'\n",
    "    '\\n Import necessary libraries. '\n",
    "    '\\n Improve efficiency, avoid too many loop nesting'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T01:45:25.507758Z",
     "iopub.status.busy": "2025-04-09T01:45:25.507331Z",
     "iopub.status.idle": "2025-04-09T01:45:25.537589Z",
     "shell.execute_reply": "2025-04-09T01:45:25.536921Z",
     "shell.execute_reply.started": "2025-04-09T01:45:25.507730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextExtractor:\n",
    "    \"\"\"Class to handle various text extraction operations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_python_code(text: str) -> List[str]:\n",
    "        pattern = r'```python\\s*(.*?)\\s*```'\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        return [matches[-1]] if matches else []\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_python_code(query: str) -> Tuple[str, int]:\n",
    "        query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n",
    "        current_rows = query.strip().split(\"\\n\")\n",
    "        new_rows = []\n",
    "        new_rows_codes = []\n",
    "        \n",
    "        for row in current_rows:\n",
    "            stripped_row = row.strip()\n",
    "            new_rows.append(row)\n",
    "            if stripped_row and not stripped_row.startswith(\"#\"):\n",
    "                new_rows_codes.append(stripped_row)\n",
    "                \n",
    "        line_count = len(new_rows_codes)\n",
    "        ans = \"\\n\".join(new_rows)\n",
    "        return ans, line_count\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_boxed_text(text: str) -> int:\n",
    "        pattern = r'oxed{(.*?)}'\n",
    "        matches = re.findall(pattern, text)\n",
    "        if not matches:\n",
    "            return -1\n",
    "        try:\n",
    "            content = matches[-1]\n",
    "            num = int(content)\n",
    "            return num % 1000\n",
    "        except ValueError:\n",
    "            try:\n",
    "                # try to convert it to int\n",
    "                num = int(float(content))\n",
    "                if math.isinf(num):\n",
    "                    print(f\"Parsed infinite value from {content}.\")\n",
    "                return num % 1000\n",
    "            except (ValueError, OverflowError):\n",
    "                return -1\n",
    "\n",
    "\n",
    "class AnswerSelector:\n",
    "    \"\"\"Class to handle answer selection logic\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def select_answer(answers: List[int]) -> int:\n",
    "        valid_answers = []\n",
    "        for answer in answers:\n",
    "            try:\n",
    "                if int(answer) == float(answer):\n",
    "                    num = int(answer)\n",
    "                    if 0 <= num < 1000:\n",
    "                        # Lower weight for numbers less than 10\n",
    "                        weight = 0.6 if num <= 20 or num%100==0 else 1\n",
    "                        # Add weighted frequency\n",
    "                        for _ in range(int(weight * 5)):\n",
    "                            valid_answers.append(num)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        if not valid_answers:\n",
    "            return 49\n",
    "            \n",
    "        # Get most frequent number\n",
    "        _, answer = sorted([(v, k) for k, v in Counter(valid_answers).items()], reverse=True)[0]\n",
    "        return answer % 1000\n",
    "\n",
    "\n",
    "class PythonREPL:\n",
    "    \"\"\"Python code execution environment\"\"\"\n",
    "    \n",
    "    def __init__(self, timeout=20):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def __call__(self, query: str) -> Tuple[bool, str]:\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n",
    "            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(query)\n",
    "            \n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"python3\", temp_file_path],\n",
    "                    capture_output=True,\n",
    "                    check=False,\n",
    "                    text=True,\n",
    "                    timeout=self.timeout,\n",
    "                )\n",
    "            except subprocess.TimeoutExpired:\n",
    "                return False, f\"Execution timed out after {self.timeout} seconds.\"\n",
    "\n",
    "            stdout = result.stdout.strip()\n",
    "            stderr = result.stderr.strip()\n",
    "\n",
    "            if result.returncode == 0:\n",
    "                return True, stdout\n",
    "            else:\n",
    "                # Process the error message to remove the temporary file path\n",
    "                error_lines = stderr.split(\"\\n\")\n",
    "                cleaned_errors = []\n",
    "                for line in error_lines:\n",
    "                    if temp_file_path in line:\n",
    "                        # Remove the path from the error line\n",
    "                        line = line.replace(temp_file_path, \"<temporary_file>\")\n",
    "                    cleaned_errors.append(line)\n",
    "                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n",
    "                # Include stdout in the error case\n",
    "                combined_output = f\"{stdout}\\n{cleaned_error_msg}\" if stdout else cleaned_error_msg\n",
    "                return False, combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T01:45:25.539028Z",
     "iopub.status.busy": "2025-04-09T01:45:25.538779Z",
     "iopub.status.idle": "2025-04-09T01:45:25.579886Z",
     "shell.execute_reply": "2025-04-09T01:45:25.579247Z",
     "shell.execute_reply.started": "2025-04-09T01:45:25.539005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import tempfile\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import time \n",
    "import sys\n",
    "import asyncio\n",
    "import ray\n",
    "import math\n",
    "sys.path.append('/kaggle/input/lmdeploy-package')\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration dataclass for model settings\"\"\"\n",
    "    model_path: str\n",
    "    gpu_indices: List[int]\n",
    "    gpu_memory_utilization: float = 0.90\n",
    "    max_model_len: int = 32768\n",
    "    cache_dir: str = None\n",
    "    quant_policy: int = 8\n",
    "    max_batch_size: int = 20\n",
    "    use_logn_attn: bool = False\n",
    "    enable_prefix_caching: bool = True\n",
    "    rope_scaling_factor: float = 1.0\n",
    "    max_prefill_token_num: int = 8192\n",
    "    num_samples: int = 20\n",
    "\n",
    "\n",
    "class LLMActor:\n",
    "    \"\"\"LLM interaction class\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, config.gpu_indices))\n",
    "        \n",
    "        self._ready = True\n",
    "        self.model_path = config.model_path\n",
    "        self.backend_config = TurbomindEngineConfig(\n",
    "            tp=len(config.gpu_indices),\n",
    "            download_dir=config.cache_dir,\n",
    "            max_batch_size=config.max_batch_size,\n",
    "            enable_prefix_caching=config.enable_prefix_caching,\n",
    "            cache_max_entry_count=config.gpu_memory_utilization,\n",
    "            session_len=config.max_model_len,\n",
    "            max_prefill_token_num=config.max_prefill_token_num,\n",
    "            quant_policy=8,\n",
    "        )\n",
    "        self.pipe = pipeline(\n",
    "            config.model_path,\n",
    "            self.backend_config\n",
    "        )\n",
    "    \n",
    "    def is_ready(self):\n",
    "        \"\"\"Check if the model is ready for inference\"\"\"\n",
    "        return self._ready\n",
    "        \n",
    "    def generate(self, texts, gen_config: GenerationConfig):\n",
    "        \"\"\"Generate completions for the given texts\"\"\"\n",
    "        response = self.pipe(\n",
    "            texts,\n",
    "            gen_config\n",
    "        )\n",
    "        return [r.text for r in response]\n",
    "    \n",
    "    async def _stop_sessions(self, pipe, start, size):\n",
    "        \"\"\"Helper method to stop model sessions\"\"\"\n",
    "        try:\n",
    "            for i in range(start, size):\n",
    "                await pipe.stop_session(i+1)\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning]: Failed to stop session {session_id+1}: {e}\", flush=True)\n",
    "    \n",
    "    async def _stop_one_session(self, pipe, session_id: int) -> None:\n",
    "        \"\"\"Helper method to stop a single model session\"\"\"\n",
    "        try:\n",
    "            await pipe.stop_session(session_id+1)\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning]: Failed to stop session {session_id+1}: {e}\", flush=True)\n",
    "\n",
    "    def _should_stop_timeout(self, valid_answers, start_time, current_speed):\n",
    "        \"\"\"Check if generation should time out based on speed setting\"\"\"\n",
    "        solve_time = time.time()\n",
    "        solved_time = solve_time - start_time\n",
    "        \n",
    "        # Base time limits\n",
    "        current_end_time = 10*60\n",
    "            \n",
    "        # Adjust timeout criteria based on speed setting\n",
    "        if current_speed <= 2:  # Fast mode - more aggressive timeouts\n",
    "            current_end_time = 9*60\n",
    "            if solved_time > 6*60 and len(valid_answers) >= 6:\n",
    "                print(\"[End] Fast mode time out with 6+ answers.\", flush=True)\n",
    "                return True\n",
    "            if solved_time > 7*60 and len(valid_answers) >= 4:\n",
    "                print(\"[End] Fast mode time out with 4+ answers.\", flush=True)\n",
    "                return True\n",
    "            if solved_time > 8*60 and len(valid_answers) >= 3:\n",
    "                print(\"[End] Fast mode time out with 3+ answers.\", flush=True)\n",
    "                return True\n",
    "        elif current_speed == 3:  # Normal mode\n",
    "            current_end_time = 10*60\n",
    "            if solved_time > 7*60 and len(valid_answers) >= 8:\n",
    "                print(\"[End] Normal mode time out with 7+ answers.\", flush=True)\n",
    "                return True\n",
    "            if solved_time > 8*60 and len(valid_answers) >= 7:\n",
    "                print(\"[End] Normal mode time out with 6+ answers.\", flush=True)\n",
    "                return True\n",
    "            if solved_time > 9*60 and len(valid_answers) >= 6:\n",
    "                print(\"[End] Normal mode time out with 5+ answers.\", flush=True)\n",
    "                return True\n",
    "        else:  # Slow mode (4-5) - more lenient timeouts\n",
    "            current_end_time = 11*60\n",
    "            if solved_time > 9*60 and len(valid_answers) >= 6:\n",
    "                print(\"[End] Slow mode time out with 6+ answers.\", flush=True)\n",
    "                return True\n",
    "            if solved_time > 10*60 and len(valid_answers) >= 5:\n",
    "                print(\"[End] Slow mode time out with 5+ answers.\", flush=True)\n",
    "                return True\n",
    "                \n",
    "        if solved_time > current_end_time or solve_time > cutoff_time:\n",
    "            print(\"[End] time out!\", flush=True)\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def _should_stop_generation(self, valid_answers, start_time, current_speed):\n",
    "        \"\"\"Determine if generation should stop based on speed settings and answer patterns\"\"\"\n",
    "        \n",
    "        # Adjust stopping criteria based on speed\n",
    "        min_answers = 10  # Default for speed 3\n",
    "        \n",
    "        if current_speed == 1:  # Fast modes\n",
    "            min_answers = 7\n",
    "        elif current_speed == 2:\n",
    "            min_answers = 9\n",
    "        elif current_speed == 4:\n",
    "            min_answers = 11\n",
    "        elif current_speed == 5:  # Slow modes\n",
    "            min_answers = 12\n",
    "        \n",
    "        # Common stopping criteria across all speeds\n",
    "        if len(valid_answers) <= 4 and any(valid_answers.count(x) >= 4 for x in valid_answers):\n",
    "            print(\"[End]: An answer repeated 4 times in less than 4 valid answers.\", flush=True)\n",
    "            return True\n",
    "        if len(valid_answers) >= 5:\n",
    "            recent_five = valid_answers[-5:]\n",
    "            if any(recent_five.count(x) >= 4 for x in recent_five):\n",
    "                print(\"[End]: An answer repeated 4 times in recent 5 valid answers.\", flush=True)\n",
    "                return True\n",
    "        if len(valid_answers) <= 6 and any(valid_answers.count(x) >= 4 for x in valid_answers):\n",
    "            print(\"[End]: An answer repeated 4 times in less than 6 valid answers.\", flush=True)\n",
    "            return True\n",
    "        if len(valid_answers) <= 8 and any(valid_answers.count(x) >= 5 for x in valid_answers):\n",
    "            print(\"[End]: An answer repeated 5 times in less than 8 valid answers.\", flush=True)\n",
    "            return True\n",
    "        if len(valid_answers) <= 9 and any(valid_answers.count(x) >= 6 for x in valid_answers):\n",
    "            print(\"[End]: An answer repeated 6 times in less than 9 valid answers.\", flush=True)\n",
    "            return True\n",
    "            \n",
    "        # Check for enough answers based on speed\n",
    "        if len(valid_answers) >= min_answers:\n",
    "            print(f\"[End]: Collected {min_answers} answers (speed={current_speed}).\", flush=True)\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "        \n",
    "    def stream_generate(self, messages, gen_config, current_speed):\n",
    "        \"\"\"Stream generation with early stopping based on specific criteria\"\"\"\n",
    "        text_extractor = TextExtractor()\n",
    "        python_repl = PythonREPL()\n",
    "        cot_answers = []\n",
    "        code_answers = []\n",
    "        valid_answers = []\n",
    "        outputs = [\"\"] * len(messages)  # Store complete output for each prompt\n",
    "        token_counts = [0] * len(messages)  # Store token count for each prompt\n",
    "        completed_status = [False] * len(messages)  # Flag to mark if each prompt is completed\n",
    "        check_token_markers = [0] * len(messages) \n",
    "        start_time = time.time()\n",
    "        session_id_start = next(self.pipe._session_id)\n",
    "        print(f\"Starting session ID: {session_id_start}\", flush=True)\n",
    "\n",
    "        try:\n",
    "            for response in self.pipe.stream_infer(messages, gen_config):  \n",
    "                if all(completed_status):  # Stop if all prompts are completed\n",
    "                    print(\"[End]: All outputs completed.\", flush=True)\n",
    "                    break\n",
    "\n",
    "                # Safely access index with error handling\n",
    "                try:\n",
    "                    index = response.index if response is not None else 0  # Get current output index\n",
    "                    if index >= len(messages):\n",
    "                        print(f\"[Warning]: Received index {index} outside range of messages ({len(messages)})\", flush=True)\n",
    "                        continue\n",
    "                        \n",
    "                    session_id = session_id_start + index\n",
    "\n",
    "                    if not completed_status[index]:\n",
    "                        # Safely append text with None check\n",
    "                        if response.text is not None:\n",
    "                            outputs[index] += response.text  # Append current token to corresponding prompt output\n",
    "                            token_counts[index] += 1  # Increment token count\n",
    "                            \n",
    "                            if self._should_stop_timeout(valid_answers, start_time, current_speed):\n",
    "                                break     \n",
    "                            # Early stopping logic\n",
    "                            # Check if </think> is found in the current output\n",
    "                            # Check for stopping criteria every 100 tokens\n",
    "                            if token_counts[index] - check_token_markers[index] >= 20:\n",
    "                                check_token_markers[index] = token_counts[index]\n",
    "                                \n",
    "                                # For chain-of-thought (index % 2 == 0), check for \\boxed{num}\n",
    "                                if index % 2 == 0 or if_only_cot:\n",
    "                                    import re\n",
    "                                    # Look for \\boxed{num} where num is an integer\n",
    "                                    boxed_pattern = re.search(r'oxed\\{(\\d+)\\}', outputs[index])\n",
    "                                    if boxed_pattern:\n",
    "                                        asyncio.run(self._stop_one_session(self.pipe, session_id))\n",
    "                                        completed_status[index] = True\n",
    "                                        current_time = time.time()\n",
    "                                        time_consumed = current_time - start_time\n",
    "                                        speed = token_counts[index]/time_consumed if time_consumed > 0 else 0\n",
    "                                        print(f\"[Early Output] {index} completed (found boxed). Time:{time_consumed:.2f}, Token:{token_counts[index]}, Speed:{speed:.2f}\", flush=True)\n",
    "                                    \n",
    "                                        # Process the output\n",
    "                                        self._process_cot_output(index, outputs[index], text_extractor, token_counts[index], \n",
    "                                                    cot_answers, valid_answers)\n",
    "                                        if self._should_stop_generation(valid_answers, start_time, current_speed):\n",
    "                                            break\n",
    "                                # For code outputs (index % 2 == 1), we need to check if we've found a complete Python code block\n",
    "                                elif index % 2 == 1:\n",
    "                                    # Check if there's a complete Python code block\n",
    "                                    if \"```python\" in outputs[index] and \"```\" in outputs[index][outputs[index].rfind(\"```python\") + 10:]:\n",
    "                                        asyncio.run(self._stop_one_session(self.pipe, session_id))\n",
    "                                        completed_status[index] = True\n",
    "                                        current_time = time.time()\n",
    "                                        time_consumed = current_time - start_time\n",
    "                                        speed = token_counts[index]/time_consumed if time_consumed > 0 else 0\n",
    "                                        print(f\"[Early Output] {index} completed (found code). Time:{time_consumed:.2f}, Token:{token_counts[index]}, Speed:{speed:.2f}\", flush=True)\n",
    "                                        \n",
    "                                        # Process the output\n",
    "                                        self._process_code_output(index, outputs[index], text_extractor, python_repl, \n",
    "                                                        token_counts[index], code_answers, valid_answers )\n",
    "                                        if self._should_stop_generation(valid_answers, start_time, current_speed):\n",
    "                                            break\n",
    "                        \n",
    "                        # Check if complete (based on finish_reason)\n",
    "                        if response.finish_reason == \"stop\" and not completed_status[index]:  \n",
    "                            completed_status[index] = True\n",
    "                            current_time = time.time()\n",
    "                            time_consumed = current_time - start_time\n",
    "                            speed = token_counts[index]/time_consumed if time_consumed > 0 else 0\n",
    "                            print(f\"[Output] {index} completed normally. Time:{time_consumed:.2f}, Token:{token_counts[index]}, Speed:{speed:.2f}\", flush=True)\n",
    "\n",
    "                            # Handle chain-of-thought output\n",
    "                            if index % 2 == 0:\n",
    "                                self._process_cot_output(index, outputs[index], text_extractor, token_counts[index], \n",
    "                                                      cot_answers, valid_answers)\n",
    "                            # Handle code output\n",
    "                            elif index % 2 == 1:\n",
    "                                self._process_code_output(index, outputs[index], text_extractor, python_repl, \n",
    "                                                        token_counts[index], code_answers, valid_answers)\n",
    "                            if self._should_stop_generation(valid_answers, start_time, current_speed):\n",
    "                                break\n",
    "                        elif response.finish_reason is not None:\n",
    "                            print(f\"[End]: Output {index} finished with reason: {response.finish_reason}.\", flush=True)\n",
    "                except (AttributeError, TypeError) as e:\n",
    "                    # Handle case where response or its attributes might be None\n",
    "                    print(f\"[Warning]: Error processing response: {e}\", flush=True)\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"[Error]: Exception during stream inference: {type(e).__name__} - {e}\", flush=True)\n",
    "        finally:\n",
    "            # Ensure we clean up even if errors occurred\n",
    "            try:\n",
    "                # Clean up model sessions\n",
    "                max_session_id = session_id_start + len(messages)\n",
    "                asyncio.run(self._stop_sessions(self.pipe, session_id_start, max_session_id+2))\n",
    "            except Exception as e:\n",
    "                print(f\"[Error]: Failed to stop sessions: {e}\", flush=True)\n",
    "                \n",
    "            # Handle any incomplete outputs\n",
    "            for i in range(len(messages)):\n",
    "                if not completed_status[i] and token_counts[i] > 0:\n",
    "                    print(f\"[Warning]: Output {i} did not complete properly. Processing anyway. Token:{token_counts[i]}\", flush=True)\n",
    "                    if i % 2 == 0:\n",
    "                        self._process_cot_output(i, outputs[i], text_extractor, token_counts[i], cot_answers, valid_answers)\n",
    "                    elif i % 2 == 1:\n",
    "                        self._process_code_output(i, outputs[i], text_extractor, python_repl, \n",
    "                                            token_counts[i], code_answers, valid_answers)\n",
    "            \n",
    "        return cot_answers, code_answers, valid_answers\n",
    "    \n",
    "    def _process_cot_output(self, index, output, text_extractor, token_count, \n",
    "                           cot_answers, valid_answers):\n",
    "        \"\"\"Process chain-of-thought output to extract answers\"\"\"\n",
    "        \n",
    "        boxed_answer = text_extractor.extract_boxed_text(output)\n",
    "        \n",
    "        if int(boxed_answer) == float(boxed_answer) and 0 < int(boxed_answer) < 1000:\n",
    "            print(f\"(Answer): extracted for Output {index}: {boxed_answer}\", flush=True)\n",
    "            cot_answers.append(boxed_answer)\n",
    "            valid_answers.append(boxed_answer)\n",
    "    \n",
    "    def _process_code_output(self, index, output, text_extractor, python_repl, token_count, \n",
    "                            code_answers, valid_answers):\n",
    "        \"\"\"Process code output to extract and potentially execute Python code\"\"\"\n",
    "        code_answer = -1\n",
    "\n",
    "        # Try to extract and execute Python code\n",
    "        python_code = text_extractor.extract_python_code(output)\n",
    "        if python_code:\n",
    "            python_code, line_count = text_extractor.process_python_code(python_code[0])\n",
    "            success, output = python_repl(python_code)\n",
    "            if success:\n",
    "                pattern = r'(\\d+)(?:\\.\\d+)?'  # Matches integers or decimals\n",
    "                matches = re.findall(pattern, output)\n",
    "                if matches:\n",
    "                    # Convert the last match to an integer\n",
    "                    try:\n",
    "                        # check if the last value is a number\n",
    "                        last_value = float(matches[-1])\n",
    "                        if math.isinf(last_value):\n",
    "                            print(f\"[Warning]: Infinite value in Python result for Output {index}: {last_value}\", flush=True)\n",
    "                        else:\n",
    "                            # Convert to int\n",
    "                            last_match = int(last_value)\n",
    "                            print(f\"<Python> result for Output {index}: {last_match}\", flush=True)\n",
    "                            code_answer = last_match % 1000\n",
    "                            if code_answer >= 0:\n",
    "                                code_answers.append(code_answer)\n",
    "                                valid_answers.append(code_answer)\n",
    "                    except OverflowError as e:\n",
    "                        print(f\"[Error]: OverflowError while processing Python result for Output {index}: {e}\", flush=True)\n",
    "                    except ValueError as e:\n",
    "                        print(f\"[Error]: ValueError while processing Python result for Output {index}: {e}\", flush=True)\n",
    "            else:\n",
    "                print(f\"[Error] code for Output {index}: {output}\", flush=True)\n",
    "        else:\n",
    "            print(f\"[No] code extracted for Output {index}.\", flush=True)\n",
    "        \n",
    "        # extract boxed answer if present\n",
    "        boxed_answer = text_extractor.extract_boxed_text(output)\n",
    "        print(f\"(Answer): extracted for Output {index}: {boxed_answer}\", flush=True)\n",
    "        if 0 <= int(boxed_answer) < 1000 and int(boxed_answer) == float(boxed_answer) and code_answer < 0:\n",
    "            code_answers.append(boxed_answer)\n",
    "            valid_answers.append(boxed_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utlities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T01:45:25.580896Z",
     "iopub.status.busy": "2025-04-09T01:45:25.580648Z",
     "iopub.status.idle": "2025-04-09T01:45:25.594168Z",
     "shell.execute_reply": "2025-04-09T01:45:25.593458Z",
     "shell.execute_reply.started": "2025-04-09T01:45:25.580872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/lmdeploy-package')\n",
    "max_round = 1\n",
    "g_score = 0\n",
    "g_count = 0\n",
    "total_avg_score = 0.0\n",
    "total_avg_length = 0.0\n",
    "total_solving_time = 0\n",
    "# Speed adjustment constants\n",
    "TOTAL_QUESTIONS = 50\n",
    "CHECK_AFTER_QUESTIONS = 30  # First check after 25 questions\n",
    "CHECK_INTERVAL = 2          # Then check every 2 questions\n",
    "TIME_THRESHOLDS = {\n",
    "    (0, 300): 1,            # < 5:00 - very fast (speed=1)\n",
    "    (300, 345): 2,          # 5:00-5:45 - fast (speed=2) \n",
    "    (345, 370): 3,          # 5:45-6:10 - normal (speed=3)\n",
    "    (370, 420): 4,          # 6:10-7:00 - slow (speed=4)\n",
    "    (420, float('inf')): 5  # > 7:00 - very slow (speed=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T01:45:25.595498Z",
     "iopub.status.busy": "2025-04-09T01:45:25.595123Z",
     "iopub.status.idle": "2025-04-09T01:45:25.655162Z",
     "shell.execute_reply": "2025-04-09T01:45:25.654472Z",
     "shell.execute_reply.started": "2025-04-09T01:45:25.595473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/lmdeploy-package')\n",
    "\n",
    "class MathSolver:\n",
    "    \"\"\"Main class to handle math problem solving\"\"\"\n",
    "    \n",
    "    def __init__(self, actor, gen_config):\n",
    "        self.actor = actor\n",
    "        self.gen_config = gen_config\n",
    "        self.text_extractor = TextExtractor()\n",
    "        self.answer_selector = AnswerSelector()\n",
    "        self.current_speed = speed\n",
    "\n",
    "    def adjust_speed(self):\n",
    "        \"\"\"Adjust speed based on progress through questions\"\"\"\n",
    "        global speed, num_samples, g_count\n",
    "        \n",
    "        # Only check at specific question counts\n",
    "        if (g_count >= CHECK_AFTER_QUESTIONS and g_count % CHECK_INTERVAL == 0 and g_count < TOTAL_QUESTIONS):\n",
    "            \n",
    "            # Calculate average time per question\n",
    "            avg_time_remain = (cutoff_time - time.time()) / (TOTAL_QUESTIONS - g_count)\n",
    "            \n",
    "            # Determine new speed based on estimated time\n",
    "            new_speed = 3  # Default\n",
    "            for time_range, speed_value in TIME_THRESHOLDS.items():\n",
    "                if time_range[0] <= avg_time_remain < time_range[1]:\n",
    "                    new_speed = speed_value\n",
    "                    break\n",
    "            \n",
    "            # Update speed if it changed\n",
    "            if new_speed != self.current_speed:\n",
    "                old_speed = self.current_speed\n",
    "                self.current_speed = new_speed\n",
    "                \n",
    "                # Update sample count based on new speed\n",
    "                global num_samples\n",
    "                num_samples = SPEED_TO_SAMPLES[new_speed]\n",
    "                \n",
    "                print(f\"[SPEED ADJUSTMENT] After {g_count} questions: remaining avg time: {avg_time_remain:.2f} minutes\")\n",
    "                print(f\"[SPEED ADJUSTMENT] Changed speed from {old_speed} to {new_speed}, num_samples={num_samples}\")\n",
    "                \n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "        \n",
    "    def predict_for_question(self, question: str, id_=None, correct_answer=None) -> int:\n",
    "        \"\"\"Predict answer for a single question\"\"\"\n",
    "        global g_score, g_count, total_solving_time\n",
    "        global total_avg_score, total_avg_length\n",
    "        \n",
    "        if time.time() > cutoff_time:\n",
    "            return 113\n",
    "        # if not id_ == \"480182\" and not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        #     return 210\n",
    "\n",
    "        # Adjust speed based on progress\n",
    "        self.adjust_speed()\n",
    "        \n",
    "        # Start timing this question\n",
    "        question_start_time = time.time()\n",
    "        \n",
    "        # Prepare questions with chain-of-thought and code prompts\n",
    "        question_cot = question + thoughts_cot\n",
    "        question_code = question + thoughts_code\n",
    "        questions = [question_cot, question_code]\n",
    "        # initial_cot = '<think>\\nOkay, so I need to solve this problem step by step and put the final answer in \\\\boxed{},'\n",
    "        # initial_code = '<think>\\nOkay, so I need to use python code to solve this problem and put the final answer in \\\\boxed{},'\n",
    "        # initial_prompts = [initial_cot, initial_code]\n",
    "        \n",
    "        print(\"correct answer:\", correct_answer, flush=True)\n",
    "        print(f\"Current speed setting: {self.current_speed}, num_samples: {num_samples}\", flush=True)\n",
    "        print(questions[0], flush=True)\n",
    "            \n",
    "        # Create messages for the model\n",
    "        list_of_messages = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": new_thoughts[k%2]},\n",
    "                {\"role\": \"user\", \"content\": questions[k%2]}\n",
    "            ] for k in range(num_samples)\n",
    "        ]\n",
    "    \n",
    "        # Generate and process model outputs\n",
    "        cot_answers, code_answers, valid_answers = self.actor.stream_generate(\n",
    "            list_of_messages, self.gen_config, self.current_speed\n",
    "        )\n",
    "        \n",
    "        # Combine and select final answer\n",
    "        valid_answers = cot_answers + code_answers\n",
    "        selected_answer = self.answer_selector.select_answer(valid_answers)\n",
    "        \n",
    "        # Print debugging information\n",
    "        print(\"cot answers:\", cot_answers, flush=True)\n",
    "        print(\"code answers:\", code_answers, flush=True)\n",
    "        print(\"all valid answers:\", valid_answers, flush=True)\n",
    "        print(\"selected answer:\", selected_answer, flush=True)\n",
    "        \n",
    "        # Calculate and store timing information\n",
    "        question_end_time = time.time()\n",
    "        question_duration = question_end_time - question_start_time\n",
    "        total_solving_time += question_duration\n",
    "        \n",
    "        # Print timing information\n",
    "        print(f\"Question {id_} solving time: {question_duration:.2f} seconds\", flush=True)\n",
    "        print(f\"Total solving time so far: {total_solving_time:.2f} seconds\", flush=True)\n",
    "        \n",
    "        g_count += 1\n",
    "        \n",
    "        return selected_answer\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Inference API function for the Kaggle competition\"\"\"\n",
    "    id_ = id_.item(0)\n",
    "    print(id_)\n",
    "    question = question.item(0)\n",
    "    prediction = math_solver.predict_for_question(question, id_)\n",
    "    return pl.DataFrame({'id': id_, 'answer': prediction})\n",
    "\n",
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-09T01:46:33.455Z",
     "iopub.execute_input": "2025-04-09T01:45:25.657308Z",
     "iopub.status.busy": "2025-04-09T01:45:25.656850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model 1...\n",
      "2025-04-09 01:45:25,831 - lmdeploy - \u001b[33mWARNING\u001b[0m - supported_models.py:106 - /kaggle/input/dpsk-14b-sft-dpo-3-16-awq-tb/keras/default/1/dpsk-14b-sft-dpo2-3-16-awq-tb seems to be a turbomind workspace, which can only be ran with turbomind engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TM][WARNING] [LlamaTritonModel] `max_context_token_num` is not set, default to 20000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n",
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n",
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n",
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n",
      "2025-04-09 01:46:04,795 - lmdeploy - \u001b[33mWARNING\u001b[0m - tokenizer.py:647 - The token <|endoftext|>, its length of indexes [27, 91, 8691, 723, 427, 91, 29] is over than 1. Currently, it can not be used as stop words\n",
      "initial_time 39.30125689506531\n",
      "71beb6\n",
      "correct answer: 902\n",
      "Current speed setting: 3, num_samples: 15\n",
      "For a positive integer $n$, let $S(n)$ denote the sum of the digits of $n$ in base 10. Compute $S(S(1)+S(2)+\\cdots+S(N))$ with $N=10^{100}-2$.\n",
      " You excel at reasoning.\n",
      " You must put the final answer in \\boxed{} before </think>.\n",
      " The final answer should modulo 1000.\n",
      " Avoid duplication and improve efficiency.\n",
      "Starting session ID: 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 获取当前环境的 PYTHONPATH\n",
    "original_pythonpath = os.environ.get(\"PYTHONPATH\", \"\")\n",
    "# 新路径\n",
    "new_path = \"/kaggle/input/lmdeploy-package\"\n",
    "# 合并原有的 PYTHONPATH 和新路径\n",
    "merged_pythonpath = f\"{new_path}:{original_pythonpath}\" if original_pythonpath else new_path\n",
    "os.environ[\"PYTHONPATH\"] = merged_pythonpath\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    gen_config = GenerationConfig(\n",
    "        temperature=0.9,\n",
    "        min_p=0.1,\n",
    "        skip_special_tokens=True,\n",
    "        max_new_tokens=16384,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "    )\n",
    "\n",
    "    model_config = ModelConfig(\n",
    "        model_path=llm_model_pth_14_3_16, \n",
    "        gpu_indices=[0,1,2,3],\n",
    "        gpu_memory_utilization=0.97, \n",
    "        max_model_len=20000, \n",
    "        quant_policy=8,\n",
    "        max_batch_size=max_batch_size,\n",
    "        num_samples=num_samples,\n",
    "    )\n",
    "    \n",
    "    print(\"loading model 1...\",flush=True)\n",
    "    actor1 = LLMActor(model_config)\n",
    "    actor1.is_ready()\n",
    "    initial_time = time.time()-global_start_time\n",
    "    print(\"initial_time\",initial_time,flush=True)\n",
    "\n",
    "    math_solver = MathSolver(actor1, gen_config)\n",
    "    \n",
    "    inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        inference_server.serve()\n",
    "    else:\n",
    "        inference_server.run_local_gateway(\n",
    "            (\n",
    "                # '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv',\n",
    "                'reference.csv',\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11376393,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "datasetId": 6554311,
     "sourceId": 10590321,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6980050,
     "sourceId": 11182463,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 205183965,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 265813,
     "modelInstanceId": 244235,
     "sourceId": 284986,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 268196,
     "modelInstanceId": 246673,
     "sourceId": 287882,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 803.741372,
   "end_time": "2024-10-27T06:12:38.713054",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-27T05:59:14.971682",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3ae3375ceefd42f9aeede66494e08172": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_712906b89a034aa981c78ec917695c16",
        "IPY_MODEL_6e02449d72574cb59a7408135ddf0606",
        "IPY_MODEL_aabe4ff8b2734c9c86a33deb7a3d0a52"
       ],
       "layout": "IPY_MODEL_d865f8a6c163451da7719a6d6e720f96"
      }
     },
     "63273838b2cf407a9d8a4b3d3c0a6096": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "64efbe72d2c24a98b72db29e311e7206": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6e02449d72574cb59a7408135ddf0606": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_805d7686fa99450fa5f28d53a2e50938",
       "max": 11,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c76df798b0e0495abc6131f26e897eca",
       "value": 11
      }
     },
     "712906b89a034aa981c78ec917695c16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_af9c9877f8c347788ca34cdd8170fc94",
       "placeholder": "​",
       "style": "IPY_MODEL_c9c855af5c4941d6bed6542ce711a012",
       "value": ""
      }
     },
     "805d7686fa99450fa5f28d53a2e50938": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aabe4ff8b2734c9c86a33deb7a3d0a52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_63273838b2cf407a9d8a4b3d3c0a6096",
       "placeholder": "​",
       "style": "IPY_MODEL_64efbe72d2c24a98b72db29e311e7206",
       "value": "Loading safetensors checkpoint shards: 100% Completed | 11/11 [04:33&lt;00:00, 26.87s/it]\n"
      }
     },
     "af9c9877f8c347788ca34cdd8170fc94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c76df798b0e0495abc6131f26e897eca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c9c855af5c4941d6bed6542ce711a012": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d865f8a6c163451da7719a6d6e720f96": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
